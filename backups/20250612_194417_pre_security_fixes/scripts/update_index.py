#!/usr/bin/env python3
"""
Update the search index with all man pages in the man_pages directory
"""

import os
import re
import json
from pathlib import Path
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

MAN_PAGES_DIR = Path("man_pages")
INDEX_FILE = Path("data/index.js")


def extract_description(text, command):
    """Extract description from man page text"""
    # Try to find NAME section
    name_match = re.search(r'NAME\s*\n+\s*([^\n]+)', text, re.IGNORECASE)
    if name_match:
        desc_line = name_match.group(1).strip()
        # Remove the command name from the beginning
        desc_line = re.sub(rf'^{re.escape(command)}\s*[-–—]\s*', '', desc_line, flags=re.IGNORECASE)
        desc_line = re.sub(rf'^[^,\s]+,\s*{re.escape(command)}\s*[-–—]\s*', '', desc_line, flags=re.IGNORECASE)
        return desc_line.strip()
    
    # Fallback: try to find DESCRIPTION section
    desc_match = re.search(r'DESCRIPTION\s*\n+\s*([^\n]+)', text, re.IGNORECASE)
    if desc_match:
        return desc_match.group(1).strip()
    
    # Last resort: first non-empty line that's not a heading
    for line in text.split('\n'):
        line = line.strip()
        if line and not line.isupper() and len(line) > 10:
            return line[:100] + "..." if len(line) > 100 else line
    
    return "Manual page"


def build_search_index():
    """Build search index from all man pages"""
    logger.info(f"Building search index from {MAN_PAGES_DIR}")
    
    index_entries = []
    processed = 0
    errors = 0
    
    # Process all .txt files in man_pages directory
    for file_path in sorted(MAN_PAGES_DIR.glob("*.txt")):
        try:
            # Parse filename: command.section.txt
            match = re.match(r'([^.]+)\.(\d+)\.txt', file_path.name)
            if not match:
                logger.warning(f"Skipping invalid filename: {file_path.name}")
                continue
            
            command, section = match.groups()
            
            # Read first part of file for description
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read(2000)  # Read first 2KB for description
            
            # Extract description
            description = extract_description(text, command)
            
            # Add to index
            index_entries.append({
                "command": command,
                "section": int(section),
                "description": description
            })
            
            processed += 1
            if processed % 100 == 0:
                logger.info(f"Processed {processed} files...")
        
        except Exception as e:
            logger.error(f"Error processing {file_path}: {e}")
            errors += 1
    
    # Sort index by command name and section
    index_entries.sort(key=lambda x: (x['command'].lower(), x['section']))
    
    # Write index file
    logger.info(f"Writing index with {len(index_entries)} entries...")
    
    # Create the JavaScript file
    js_content = f"""// Search index for man pages
// Generated by update_index.py
// Total entries: {len(index_entries)}

window.searchIndex = {json.dumps(index_entries, indent=2)};
"""
    
    # Ensure data directory exists
    INDEX_FILE.parent.mkdir(exist_ok=True)
    
    # Write the index
    with open(INDEX_FILE, 'w', encoding='utf-8') as f:
        f.write(js_content)
    
    logger.info(f"Index updated successfully!")
    logger.info(f"Total entries: {len(index_entries)}")
    logger.info(f"Processed: {processed}, Errors: {errors}")
    
    # Show some statistics
    sections = {}
    for entry in index_entries:
        section = entry['section']
        sections[section] = sections.get(section, 0) + 1
    
    logger.info("\nEntries by section:")
    for section in sorted(sections.keys()):
        logger.info(f"  Section {section}: {sections[section]} entries")


def verify_index():
    """Verify the index file is valid"""
    try:
        with open(INDEX_FILE, 'r') as f:
            content = f.read()
        
        # Extract the JSON part
        json_match = re.search(r'window\.searchIndex = (\[.*\]);', content, re.DOTALL)
        if json_match:
            index_data = json.loads(json_match.group(1))
            logger.info(f"Index verification passed: {len(index_data)} entries")
            return True
        else:
            logger.error("Could not find searchIndex in file")
            return False
    
    except Exception as e:
        logger.error(f"Index verification failed: {e}")
        return False


def main():
    # Check if man_pages directory exists
    if not MAN_PAGES_DIR.exists():
        logger.error(f"Directory {MAN_PAGES_DIR} not found!")
        return 1
    
    # Count man pages
    man_page_count = len(list(MAN_PAGES_DIR.glob("*.txt")))
    logger.info(f"Found {man_page_count} man page files")
    
    if man_page_count == 0:
        logger.error("No man page files found!")
        return 1
    
    # Build the index
    build_search_index()
    
    # Verify the result
    if verify_index():
        logger.info("\nIndex update completed successfully!")
        return 0
    else:
        logger.error("\nIndex update failed!")
        return 1


if __name__ == "__main__":
    exit(main())